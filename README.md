# Flex:ai 介绍及使用说明  

## 概述

Flex:ai是一个面向AI容器场景的开源项目，其核心能力包含两大部分，分别是XPU虚拟化和多级智能调度。其中XPU虚拟化分为本地XPU虚拟化和跨节点拉远虚拟化，本地XPU虚拟化支持将1台服务器上的XPU算力卡虚拟化分割出多个虚拟算力单元，实现单张算力卡在多个容器的共享。跨节点拉远虚拟化支持通过RDMA或TCP网络访问远端节点上的算力卡。多级智能调度支持对切分出的虚拟卡进行Binpack等资源级调度，以及对AI训推任务进行分时调度等能力。

## 动机

用户的AI业务集群通常是大小模型混部的场景，集群除了运行AI大模型如Deepseek、Qwen系列，还有大量的小参数量模型，如CV模型、Embedding模型等，这些小参数量模型无法充分使用整张算力卡的资源，从而导致昂贵的GPU/NPU资源存在浪费。此外算力集群中运行的AI工作负载千差万别，如何基于有限的算力资源对大并发的AI工作负载进行高效调度是一个难题。基于上述的背景，我们构筑了Flex:ai开源项目，提供将XPU算力卡进行虚拟化切分，以及面向AI训推任务和集群资源做智能调度的能力。

# Ascend NPU虚拟化

请查看[Ascend-Virtual-Service](Ascend-Virtual-Service/)

# GPU虚拟化

请查看[GPU-Virtual-Service](GPU-Virtual-Service/)
